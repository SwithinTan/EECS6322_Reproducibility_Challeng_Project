{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go5hoYWvq8cZ"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from gandetect import LOG\n",
        "\n",
        "\n",
        "class Training(object):\n",
        "    \"\"\"Training wrapper for training models. Supports early stopping, saving the current best model and loading it on halt..\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 training_data,\n",
        "                 epochs=50,\n",
        "                 batch_size=128,\n",
        "                 optimizer=torch.optim.Adam,\n",
        "                 learning_rate=1e-4,\n",
        "                 loss=torch.nn.BCEWithLogitsLoss,\n",
        "                 print_steps=2_000,\n",
        "                 use_cuda=True,\n",
        "                 multi_gpu=False,\n",
        "                 num_workers=multiprocessing.cpu_count(),\n",
        "                 patience=5,\n",
        "                 early_stopping_method=\"simple\",\n",
        "                 weight_decay=0,\n",
        "                 tensorboard=True,\n",
        "                 save_logits_histograms=False,\n",
        "                 save_path=\"experiments\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model (gandetect.model): The model to train.\n",
        "            training_data (torch.data.Dataset): A pytorch dataset holding the training data.\n",
        "            epochs (int): Epochs to train for.\n",
        "            batch_size (int): Batch size to use.\n",
        "            optimizer (torch.optim.Optimizer): An instance of an optimizer class.\n",
        "            loss (torch.nn.Loss): An instance of a loss class.\n",
        "            print_steps (int): Print progress every x iterations.\n",
        "            use_cuda (bool): Use GPU backend if available?\n",
        "            multi_gpu (bool): Use multiple GPU if available?\n",
        "            num_workers (int): Worker threads used for loading data.\n",
        "            patience (int): Patience for early stopping.\n",
        "            early_stopping_method (str): Method to use for ealry stopping:\n",
        "                - simple (default): Stop if validation criteria does not improve after patience epochs.\n",
        "                - patience: The learning rate gets lowered by 10x when the validation accuracy does not improve for patience epochs. \n",
        "                Terminate when learning rate reaches 10e-6.\n",
        "            weight_decay (float): L2 regularization parameter.\n",
        "            tensorboard (bool): Save results using tensorboard?\n",
        "            save_logits_histograms (bool): Save histograms of the logits layer.\n",
        "            save_path (str): Path to save tensorboard logs and model to.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda\")\n",
        "            self.multi_gpu = multi_gpu\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "\n",
        "        # general parameters\n",
        "        if multi_gpu and (torch.cuda.device_count() > 1):\n",
        "            LOG.info(f\"Training on {torch.cuda.device_count()} GPUs!\")\n",
        "            model = torch.nn.DataParallel(model)\n",
        "            batch_size *= torch.cuda.device_count()\n",
        "\n",
        "        self.model = model.to(self.device)\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "        # setup model directory and logging\n",
        "        self.save_path = save_path\n",
        "        if not os.path.exists(self.save_path):\n",
        "            os.makedirs(self.save_path)\n",
        "\n",
        "        self.model_path = f\"{self.save_path}/checkpoint.pth\"\n",
        "\n",
        "        self.tensorboard = tensorboard\n",
        "        self.save_logits_histograms = save_logits_histograms\n",
        "        if self.tensorboard:\n",
        "            self.writer = SummaryWriter(self.save_path)\n",
        "\n",
        "        # early stopping\n",
        "        self.patience = patience\n",
        "        self.best_eval = None\n",
        "        self.counter = 0\n",
        "\n",
        "        # setup training\n",
        "        if len(training_data) == 0:\n",
        "            raise ValueError(\"Provided empty dataset!\")\n",
        "        split = int(len(training_data) * 0.9)\n",
        "        self.train_data, self.val_data = torch.utils.data.random_split(\n",
        "            training_data, [split, len(training_data) - split])\n",
        "\n",
        "        LOG.info(f\"Using {len(self.val_data)} images for validation!\")\n",
        "\n",
        "        # setup training loader\n",
        "        self.train_loader = torch.utils.data.DataLoader(\n",
        "            self.train_data, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
        "\n",
        "        # setup optimzer\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.optimizer_fn = optimizer\n",
        "        self.early_stopping_method = early_stopping_method\n",
        "        self._init_optimizer()\n",
        "\n",
        "        self.loss = loss()\n",
        "        self.print_steps = print_steps\n",
        "\n",
        "        # setup statistics\n",
        "        self.epoch_loss = None\n",
        "        self.epoch_acc = None\n",
        "\n",
        "    def _init_optimizer(self):\n",
        "        self.optimizer = self.optimizer_fn(params=self.model.parameters(),\n",
        "                                           lr=self.learning_rate, weight_decay=self.weight_decay)\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Train the given model until either max epochs is reached or early stopping criteria is fullfilled.\"\"\"\n",
        "        self.epoch_loss = list()\n",
        "        self.epoch_acc = list()\n",
        "        self.model = self.model.train()\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            running_loss = 0.0\n",
        "            epoch_loss = 0.0\n",
        "\n",
        "            for i, (x, y) in enumerate(self.train_loader):\n",
        "                x, y = x.to(self.device), y.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # predict and optimize\n",
        "                logits = self.model(x)\n",
        "                loss = self.loss(logits.view(-1), y.double())\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if i % self.print_steps == self.print_steps - 1:\n",
        "                    print(\n",
        "                        f\"Epoch {epoch+1:03d} [{i+1: 5d}]: Running {running_loss / self.print_steps: .6f}, Current {loss.item(): .6f}\\r\", end=\"\")\n",
        "                    running_loss = 0.0\n",
        "                    if self.save_logits_histograms:\n",
        "                        self.writer.add_histogram(\n",
        "                            \"Train/LogitsDist\", logits, (epoch * 1_000) + i)\n",
        "\n",
        "            epoch_loss = epoch_loss / i\n",
        "            LOG.info(\n",
        "                f\"Epoch {epoch+1:03d}: Loss: {epoch_loss}\")\n",
        "            self.epoch_loss.append(epoch_loss)\n",
        "\n",
        "            acc = self.evaluate(self.val_data, current_epoch=epoch)\n",
        "            self.epoch_acc.append(acc)\n",
        "\n",
        "            if self.tensorboard:\n",
        "                self.writer.add_scalar(\n",
        "                    \"Loss/train\", epoch_loss, epoch)\n",
        "                self.writer.add_scalar(\n",
        "                    \"Acc/val\", acc, epoch)\n",
        "\n",
        "            if self._early_stopping(acc):\n",
        "                LOG.info(\n",
        "                    f\"Early Stopping after {epoch+1} epoch(s): best accuracy: {self.best_eval:.2f}; current: {acc:.2f}!\")\n",
        "                self.load_model()\n",
        "                return\n",
        "\n",
        "    def _early_stopping(self, eval_criteria):\n",
        "        if self.best_eval is None or self.best_eval < eval_criteria:\n",
        "            self.best_eval = eval_criteria\n",
        "            self.counter = 0\n",
        "            self.save_model()\n",
        "\n",
        "        if self.patience <= self.counter:\n",
        "            if self.early_stopping_method == \"simple\":\n",
        "                return True\n",
        "            elif self.early_stopping_method == \"patience\":\n",
        "                # modify learning rate\n",
        "                self.counter = 0\n",
        "\n",
        "                for param_group in self.optimizer.param_groups:\n",
        "                    param_group[\"lr\"] /= 10\n",
        "\n",
        "                    # if we reach abort criteria\n",
        "                    if param_group[\"lr\"] < 1e-6:\n",
        "                        return True\n",
        "\n",
        "                    LOG.info(\n",
        "                        f\"Reducing learning rate to {param_group['lr']:e}!\")\n",
        "\n",
        "        self.counter += 1\n",
        "\n",
        "        return False\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Save the model\"\"\"\n",
        "        torch.save(self.model, self.model_path)\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load model\"\"\"\n",
        "        torch.load(self.model_path)\n",
        "\n",
        "    def evaluate(self, test, current_epoch=None):\n",
        "        \"\"\"Evaluate the model on the test data. Measures accuracy.\"\"\"\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "            test, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "        total = 0\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.model = self.model.eval()\n",
        "            for i, (x, y) in enumerate(test_loader):\n",
        "                x, y = x.to(self.device), y.to(self.device)\n",
        "                outputs = self.model(x)\n",
        "                predicted = (torch.sigmoid(outputs) > 0.5).int()\n",
        "\n",
        "                # save logits dist during training\n",
        "                if current_epoch is not None and self.tensorboard and self.save_logits_histograms:\n",
        "                    self.writer.add_histogram(\"Test/LogitsDist\", outputs,\n",
        "                                              (current_epoch * 1_000) + i)\n",
        "\n",
        "                    self.model = self.model.train()\n",
        "                    self.writer.add_histogram(\"Test/LogitsDistInTrainMode\", self.model(x),\n",
        "                                              (current_epoch * 1_000) + i)\n",
        "                    self.model = self.model.eval()\n",
        "\n",
        "                total += y.size(0)\n",
        "                correct += (predicted.t() == y).sum().cpu().item()\n",
        "            self.model = self.model.train()\n",
        "\n",
        "        return correct / total * 100.\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, _type, _value, _traceback):\n",
        "        if self.tensorboard:\n",
        "            self.writer.close()"
      ]
    }
  ]
}