EECS6322 Reproducibility Challenge Final Report

## Main Contribution

In the paper, the main contributions are:

1.  They introduced a new approach to detect CNN-generated images, by training forensics models on CNN-generated images. This approach shows a surprising amount of generalization compared with other CNN systhesis methods. 
2. They introduced a new dataset and evaluation metrics for detecting CNN-generated images.
3. They experimentally analyzed the factors that account for cross-model generalization.





## About My Reproducibility Attempt

There are two main attempts in my reproduction project that correspond to the two main contributions of the original paper:

1. The first attempt is to verify if the data augmentations improve generalization.
2. The second attemp is to verify if more diverse datasets improve generalization on unseen architectures.

First, I will try to verify the first and then the second if time allows.

### Data Augmentations Improve Generalization

1. Dataset

   For the training, the training dataset is consist of original images and fake images generated by CNN generator, ProGAN. 

   The training dataset used in the paper can be downloaded via [this google drive link](https://drive.google.com/file/d/1iVNBV0glknyTYGA9bCxT_d0CVTOgGcKh/view).

2. Models

   Firstly, I want to verify the first claim - whether data augmentation can improve the generalization or not. I trained a series of different models with four different image augmentation techniques with one baseline.

   - Gaussian blur (Blur): Images from the original dataset have 50% probablity to be blurred.
   - JPEG-compressing (JPEG): Images from the dataset have 50% probablity to be JPEG-compressed by two image libraries, OpenCV and PIL.
   - Gaussian blur + JPEG-compreesing (Blur + JPEG (0.5)): Images from the dataset have 50% probability to be blurred and JPEG-compressed. (The two are independent from each other.)
   - Gaussian blur + JPEG-compressing (Blur + JPEG (0.1)): Images from the dataset have 10% probability to be blurred and JPEG-compressed. (The two are independent from each other.)
   - No Augmentation (No Aug): As a baseline, I also trained one more model on the images without any augmentation.

3. Methodology

   I trained 5 models in total. T

   

   

   

4. Training Details

   I sampled 10% of the training data as validation data. I trained all models with batch size of 64, and an initial learning rate at 10e-4. The learning rate will drop by 10 times after the validation accuracy stagnates for 5 epochs.

### 









### Diverse Datasets Improve Generalization

1. Dataset

   For the training, the training dataset is consist of original images and fake images generated by CNN generator, ProGAN. 

   The training dataset used in the paper can be downloaded via [this google drive link](https://drive.google.com/file/d/1iVNBV0glknyTYGA9bCxT_d0CVTOgGcKh/view).

2. Models

   Here, I want to verify the second claim - whether a more diverse training dataset can improve the generalization or not. I trained a series of different models with four different image augmentation techniques with one baseline.

   - 1 class: 
   - 3 class:
   - 5 class:

3. Methodology

   I trained 5 models in total. T









## Discussion

#### Difficulties

1. Dataset downloader provided by the author has been deprecated, and the size of dataset is relatively large (70GB), which causes some trouble when I was trying to ingest dataset. This problem has been solved by downloading original datasets and save to another drive. 

#### Future

1. For now, I have only tested on one classification model, ResNet50. As an result, the conclusion that is drawn form this research could be dependent on the classifier. However, due to the limit of time and computation power, I haven't tried other classifiers. In the future research, other classification models like VGG could be introduced and test if the result is independent from the kind of classification model that used or not. 