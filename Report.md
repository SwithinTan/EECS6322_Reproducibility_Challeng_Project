EECS6322 Reproducibility Challenge Final Report

## Main Contribution

In the paper, the main contributions are:

1.  They introduced a new approach to detect CNN-generated images, by training forensics models on CNN-generated images. This approach shows a surprising amount of generalization compared with other CNN systhesis methods. 
2. They introduced a new dataset and evaluation metrics for detecting CNN-generated images.
3. They experimentally analyzed the factors that account for cross-model generalization.





## About My Reproducibility Attempt

There are two main attempts in my reproduction project that correspond to the two main contributions of the original paper:

1. The first attempt is to verify if the data augmentations improve generalization.
2. The second attemp is to verify if more diverse datasets improve generalization on unseen architectures.

First, I will try to verify the first and then the second if time allows.

### Claim 1: Data Augmentations Improve Generalization

1. Dataset

   For the training, the training dataset is consist of original images and fake images generated by CNN generator, ProGAN. 

   The training dataset used in the paper can be downloaded via [this google drive link](https://drive.google.com/file/d/1iVNBV0glknyTYGA9bCxT_d0CVTOgGcKh/view?usp=share_link).

2. Models

   Firstly, I want to verify the first claim - whether data augmentation can improve the generalization or not. I trained a series of different models with four different image augmentation techniques with one baseline.

   - Gaussian blur (Blur): Images from the original dataset have 50% probablity to be blurred.
   - JPEG-compressing (JPEG): Images from the dataset have 50% probablity to be JPEG-compressed by two image libraries, OpenCV and PIL.
   - Gaussian blur + JPEG-compreesing (Blur + JPEG (0.5)): Images from the dataset have 50% probability to be blurred and JPEG-compressed. (The two are independent from each other.)
   - Gaussian blur + JPEG-compressing (Blur + JPEG (0.1)): Images from the dataset have 10% probability to be blurred and JPEG-compressed. (The two are independent from each other.)
   - No Augmentation (No Aug): As a baseline, I also trained one more model on the images without any augmentation.

   There are 5 models that were trained with training datasets of four image augmentation and one more dataset without any augmentation as a baseline model. 

   The 5 models that I trained are now on [google drive](https://drive.google.com/drive/folders/1uM2ZBbiCc_j8UXzEJdLsH5G6Y0YWUFu5?usp=share_link), and can be downloaded. 

3. Training Details

   I sampled 10% of the training data as validation data. I trained all models with batch size of 64, and an initial learning rate at 10e-4. The learning rate will drop by 10 times after the validation accuracy stagnates for 5 epochs. I used Google colab for training and the Tesla T4 gpu for acceleration. For each model, it took around 8 hours for training.

4. Model Evaluation

   ![image-20230414015229227](/Users/swithin/Library/Application Support/typora-user-images/image-20230414015229227.png)





### Claim 2: Diverse Datasets Improve Generalization

1. Dataset

   For the training, the training dataset is consist of original images and fake images generated by CNN generator, ProGAN. 

   The training dataset used in the paper can be downloaded via [this google drive link](https://drive.google.com/drive/folders/1EBSAe6GIlvd8vHaAQHET5REbu7ObZ_Sx?usp=share_link).

2. Models

   Here, I want to verify the second claim - whether a more diverse training dataset can improve the generalization or not. I trained a series of different models with four different image augmentation techniques with one baseline.

   - 1 class: Airplane image dataset.
   - 3 class: Airplane, person and cat image datasets.
   - 5 class: Airplane, person, cat, potted plant, and tv monitor image datasets.

   There are 3 models that trained with different volume of training datasets, one, three and five datasets. 

   The 3 models that I trained are now on [google drive](https://drive.google.com/drive/folders/1EBSAe6GIlvd8vHaAQHET5REbu7ObZ_Sx?usp=share_link), and can be downloaded. 

3. Training Details

   I sampled 10% of the training data as validation data. I trained all models with batch size of 64, and an initial learning rate at 10e-4. The learning rate will drop by 10 times after the validation accuracy stagnates for 5 epochs. I used Google colab and the Tesla T4 gpu for acceleration. The training time for the three models are 7 hours, 10 hours, 15 hours respectively.

4. Model Evaluation

   







## Discussion

#### Difficulties

1. Data Availability: Dataset downloader provided by the author has been deprecated, and the size of dataset is relatively large (~70GB), which causes some trouble when I was trying to ingest dataset. This problem has been solved by downloading original datasets and saving to another drive. 
2. Limited Computation Power and long training time: As the training dataset is huge, the training time increases. 

#### Future

1. For now, I have only tested on one classification model, ResNet50. As an result, the conclusion that is drawn form this research could be dependent on the classifier. However, due to the limit of time and computation power, I haven't tried other classifiers. In the future research, other classification models like VGG could be introduced and test if the result is independent from the kind of classification model that used or not. 
2. In this reproduction project, and in the verfication of claim 1, I only used one class - airplane for training   and comparing 5 trained models. That could be the reason why the verification of claim 1 is not successful compared with the verfication of claim 2. Because it's supposed to be using all 20 classes for the 5 models' training. I tried to train one model with all 20 classes, however, the program crashed after 20 hours training. I will try to redesign and reimplement this reproduction project if computation power and time allows in the future.